# === IMPORTS ===
import logging
import os
import re
import json
import uuid
import numpy as np
import functools
from typing import List
from datetime import datetime
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from openai import OpenAIError  # para capturar errores de OpenAI
import openai
from httpx import HTTPError
from logging.handlers import RotatingFileHandler
from prometheus_client import Histogram, start_http_server
from conexion import conectar_supabase
from circuitbreaker import circuit

# === CONFIGURACIÃ“N GENERAL ===
TABLA_EMBEDDINGS = os.getenv("TABLA_EMBEDDINGS", "documentos_embeddings")
# Si prefieres usar el modelo de SentenceTransformers (local) puedes dejarlo asÃ­:
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
# Si deseas usar el modelo de OpenAI para embeddings, deberÃ¡s ajustar la lÃ³gica en las funciones de vectorizaciÃ³n.
TOP_K = int(os.getenv("TOP_K", 3))
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "TU_API_KEY")

# === CONFIGURAR LOGGING ===
def configurar_logging(nombre_modulo: str):
    logger = logging.getLogger(nombre_modulo)
    if logger.handlers:
        return
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
    log_filename = f"{nombre_modulo}.log"
    file_handler = RotatingFileHandler(log_filename, maxBytes=5*1024*1024, backupCount=3)
    file_handler.setFormatter(formatter)
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)

configurar_logging("ia")
logger = logging.getLogger("ia")

# === MÃ‰TRICAS PROMETHEUS ===
GPT_RESPONSE_LATENCY = Histogram('gpt_response_latency_seconds', 'Tiempo de respuesta del modelo de IA')
SIMILITUD_SCORE = Histogram('similitud_score', 'Puntajes de similitud')
CONTEXTO_LENGTH = Histogram('contexto_length', 'Longitud del contexto en palabras')

# === CARGAR VARIABLES DE ENTORNO ===
load_dotenv()
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
openai.api_key = OPENAI_API_KEY

# === MODELO DE EMBEDDINGS (singleton) ===
_modelo = None
def get_modelo():
    global _modelo
    if _modelo is None:
        _modelo = SentenceTransformer(EMBEDDING_MODEL)
        logger.info(f"âœ… Modelo de embeddings cargado: {EMBEDDING_MODEL}")
    return _modelo

# === SANITIZACIÃ“N DE PREGUNTA ===
def sanitizar_pregunta(pregunta: str, max_length=500) -> str:
    # Se eliminan caracteres no alfanumÃ©ricos (permitiendo acentos, Ã± y signos de interrogaciÃ³n)
    pregunta = re.sub(r'[^\w\sÃ¡Ã©Ã­Ã³ÃºÃ±Â¿?]', '', pregunta.strip())
    return pregunta[:max_length]

# === VECTORIZACIÃ“N DE PREGUNTA ===
def vectorizar_pregunta(pregunta: str) -> List[float]:
    try:
        modelo = get_modelo()
        # Codifica la pregunta a un vector NumPy
        return modelo.encode([pregunta], convert_to_numpy=True)[0]
    except Exception as e:
        logger.error(f"âŒ Error vectorizando pregunta: {e}")
        raise

# === SIMILITUD COSENO ===
def similitud_coseno(v1: List[float], v2: List[float]) -> float:
    v1 = np.array(v1)
    v2 = np.array(v2)
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)
    if norm_v1 == 0 or norm_v2 == 0:
        return 0.0
    return np.dot(v1, v2) / (norm_v1 * norm_v2)

# === TRUNCAR CONTEXTO ===
def truncar_contexto(contexto: str, max_palabras: int = 1500) -> str:
    palabras = contexto.split()
    if len(palabras) > max_palabras:
        logger.warning("âš ï¸ Contexto truncado por exceso de longitud.")
        return " ".join(palabras[:max_palabras])
    CONTEXTO_LENGTH.observe(len(palabras))
    return contexto

# === BÃšSQUEDA DE CONTEXTO CON RPC VECTORIAL ===
def obtener_contexto_relevante(pregunta: str, supabase, k=TOP_K) -> str:
    try:
        logger.info("ğŸ” Recuperando contexto relevante...")
        pregunta_vector = vectorizar_pregunta(pregunta).tolist()
        response = supabase.rpc('vector_search', {
            'query_embedding': pregunta_vector,
            'match_count': k * 2
        }).execute()

        if not response.data:
            logger.warning("âš ï¸ No se encontraron documentos relevantes.")
            return ""

        resultados = []
        for doc in response.data:
            embedding = doc.get("embedding")
            if isinstance(embedding, list) and embedding:
                score = similitud_coseno(pregunta_vector, embedding)
                SIMILITUD_SCORE.observe(score)
                resultados.append((score, doc))

        resultados.sort(key=lambda x: x[0], reverse=True)
        top_k_docs = [doc["contenido"] for _, doc in resultados[:k]]
        logger.info(f"ğŸ“š Top-{k} documentos seleccionados como contexto.")
        return truncar_contexto("\n\n".join(top_k_docs))
    except Exception as e:
        logger.error(f"âŒ Error al recuperar contexto: {e}")
        raise

# === GPT (con Prometheus y circuit breaker) ===
@circuit(failure_threshold=3, recovery_timeout=60)
@GPT_RESPONSE_LATENCY.time()
def responder_con_gpt(pregunta: str, contexto: str) -> str:
    try:
        system_prompt = (
            "Eres un asistente de administraciÃ³n de fincas. Responde solo en base a la informaciÃ³n del contexto. "
            "Si no encuentras respuesta, indica amablemente que no tienes informaciÃ³n."
        )
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Contexto:\n{contexto}\n\nPregunta: {pregunta}"}
        ]
        completion = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.2,
            timeout=15
        )
        return completion.choices[0].message.content.strip()
    except OpenAIError as e:
        logger.error(f"âŒ Error al generar respuesta con GPT: {e}")
        raise

# === LRU CACHE LOCAL DE RESPUESTAS ===
_respuestas_cache = {}

def responder_pregunta(pregunta: str, user_id: str = None) -> str:
    session_id = str(uuid.uuid4())[:8]
    pregunta = sanitizar_pregunta(pregunta)
    if not pregunta:
        return "Por favor, formula una pregunta vÃ¡lida."

    logger.info(f"ğŸ†” SesiÃ³n: {session_id} {'(Usuario: ' + user_id + ')' if user_id else ''}")

    if pregunta in _respuestas_cache:
        logger.info(f"âš¡ [{session_id}] Respuesta recuperada de cachÃ©.")
        return _respuestas_cache[pregunta]

    try:
        supabase = conectar_supabase()
        contexto = obtener_contexto_relevante(pregunta, supabase)
        respuesta = responder_con_gpt(pregunta, contexto)
        logger.info(f"âœ… [{session_id}] Respuesta generada.")
        _respuestas_cache[pregunta] = respuesta
        return respuesta
    except Exception as e:
        logger.error(f"ğŸš¨ [{session_id}] Error: {e}")
        return "Hubo un problema al procesar tu solicitud. Intenta mÃ¡s tarde."

# === TEST MANUAL ===
if __name__ == "__main__":
    try:
        start_http_server(8010)  # Inicia el servidor de mÃ©tricas en el puerto 8010
        pregunta = input("â“ Escribe tu pregunta: ")
        respuesta = responder_pregunta(pregunta)
        print(f"ğŸ§  Respuesta:\n{respuesta}")
    except Exception as e:
        logger.error(f"âŒ Error en ejecuciÃ³n directa: {e}")
